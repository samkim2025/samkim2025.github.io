<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS184 Project 3 - PathTracer</title>
    <style>
        :root {
            --primary-color: #4a148c;
            --secondary-color: #6a1b9a;
            --accent-color: #9c27b0;
            --text-color: #2d3748;
            --bg-color: #f8f9fa;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.8;
            margin: 40px auto;
            padding: 0 40px;
            max-width: 1200px;
            background-color: var(--bg-color);
            color: var(--text-color);
        }

        h1 {
            color: var(--primary-color);
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 1.5em;
            border-bottom: 3px solid var(--accent-color);
            padding-bottom: 0.5em;
        }

        h2 {
            color: var(--secondary-color);
            margin-top: 2em;
            font-size: 1.8em;
            border-left: 4px solid var(--accent-color);
            padding-left: 15px;
        }

        .image-container {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            margin-bottom: 40px;
            transition: transform 0.3s ease;
        }

        .image-container:hover {
            transform: translateY(-5px);
        }

        img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            display: block;
            margin: 20px auto;
        }

        p {
            text-align: justify;
            margin-bottom: 1.5em;
        }

        .algorithm-steps {
            background-color: #f3e5f5;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }

        .grid-container {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin-top: 20px;
        }

        .grid-2x2 {
            grid-template-columns: repeat(2, 1fr);
        }

        .code {
            font-family: 'Courier New', Courier, monospace;
            background-color: #f8f9fa;
            padding: 2px 4px;
            border-radius: 3px;
        }

        @media (max-width: 768px) {
            body {
                padding: 20px;
            }
            
            .grid-container, .grid-2x2 {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <h1>CS184 Project 3: Ray Pathtracer</h1>
    <h2>https://samkim2025.github.io/cs184/3/index.html</h2>

    <h2>Overview</h2>
    <p>In this assignment, I implemented core algorithms for Bézier curve and surface evaluation, vertex normal computation, and mesh subdivision techniques, including edge flips, splits, and Loop subdivision. The Bézier curve and patch implementations use de Casteljau’s algorithm to recursively compute intermediate points, ensuring smooth interpolation. The vertex normal calculation uses area-weighted triangle normals to provide a better approximation of smooth shading. The mesh subdivision methods allow for refining geometry while maintaining consistency, leveraging edge flips and splits for uniformity. Through this work, I gained insight into the mathematical foundations of geometric modeling and the impact of different data structures on computational efficiency and accuracy.</p>
    
    <h2>Part 1: Ray Generation and Scene Intersection</h2>
    <div class="algorithm-steps">
        <p><strong>1. Walk through the ray generation and primitive intersection parts of the rendering pipeline.</strong><br>
        <ul>
            <li>Ray Generation: Using <span class="code">raytrace_pixel()</span>,   I generate  multiple sample rays through different points within the pixel for each pixel in the image. I determine the sample positions using a grid sampler that provides random offsets within the pixel. Each raw pixel coordinate (x,y) is then converted to normalized camera space coordinates in the range [0,1]×[0,1]. 
                The camera finally generates rays from these normalized coordinates using <span class="code">camera->generate_ray()</span>, which creates rays that originate from the camera position and pass through the corresponding point on the image plane.</li>
            <li>Scene Traversal: Each ray is cast into the scene to find intersections with scene primitives. The est_radiance_global_illumination() function traces the ray through the scene. The BVH (Bounding Volume Hierarchy) acceleration structure is used to efficiently find potential intersections.</li>
            <li>Primitive Intersection: Each ray is tested against primitives (triangles and spheres) to find the closest intersection point. Rays have parameters <span class="code">min_t</span> and <span class="code">max_t</span> that define the valid range for intersections. When an intersection is found, the ray's <span class="code">max_t</span>max_t is updated to this intersection distance, ensuring that future intersection tests only consider closer objects. 
                For each valid intersection, information such as the intersection point, surface normal, and material properties is stored.</li>
            <li>Shading: In the initial implementation, I used a simple debug visualization with the surface normal as color.</li>
        </ul>
        <p><strong>2.  Explain the triangle intersection algorithm you implemented in your own words.</strong><br>
        <ul>
            <li>The algorithm starts by computing two edge vectors of the triangle: <span class="code">e1 = p2 - p1</span> and <span class="code">e2 = p3 - p1</span>. These define the triangle's plane and orientation.</li>
            <li>I calculated the determinant <span class="code">a</span> using the cross product of the ray direction and one edge: <span class="code">h = cross(r.d, e2)</span> and <span class="code">a = dot(e1, h)</span>. This determinant tells us if the ray is parallel to the triangle (if near zero) or at what angle it intersects.</li>
            <li>The algorithm directly computes barycentric coordinates <span class="code">u</span> and <span class="code">v</span> (with <span class="code">w = 1-u-v</span> implied), which tell us if the intersection point is inside the triangle and allow us to interpolate vertex attributes (like normals) at the intersection point.</li>
            <li>For a valid intersection, we need the ray and triangle not to be parallel (|a| > epsilon), barycentric coordinates to be within bounds (<span class="code">0 ≤ u</span>, <span class="code">v ≤ 1</span> and <span class="code">u+v ≤ 1</span>), and the intersection distance <span class="code">t</span> to be within the ray's valid range <span class="code">(min_t ≤ t ≤ max_t)</span>.</li>
            <li>If all checks pass, we interpolate the vertex normals using the barycentric coordinates to get the surface normal at the intersection point: <span class="code">isect->n = (w * n1 + u * n2 + v * n3).unit();</span></li>
        </ul>
        <p><strong>3. Show images with normal shading for a few small .dae files.</strong><br>
        <div class="grid-container grid-2x2">
            <div class="image-container">
                <h3>CB Empty</h3>
                <img src="media/1.2.png" alt="Empty">
            </div>
            <div class="image-container">
                <h3>CB Spheres</h3>
                <img src="media/1.1.png" alt="Spheres">
            </div>
        </div>
        <div class="grid-container grid-2x2">
            <div class="image-container">
                <h3>CB Gems</h3>
                <img src="media/1.3.png" alt="Gems">
            </div>
            <div class="image-container">
                <h3>CB Coil</h3>
                <img src="media/1.4.png" alt="Coil">
            </div>
        </div>
    </div>
    
    <h2>Part 2: Bounding Volume Hierarchy</h2>
    <div class="algorithm-steps">
        <p><strong>1. Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.</strong><br>
        <ul>
            <li>For each node, I first compute a bounding box that includes all primitives in the current set.</li>
            <li>If the number of primitives is less than or equal to <span class="code">max_leaf_size</span> (4 by default), I create a leaf node that stores references to these primitives.</li>
            <li>For internal nodes, I select the axis with the largest extent (x, y, or z) for splitting. This approach tends to create more balanced trees by splitting along the dimension where primitives are most spread out.</li>
            <li>Next, I use the average of centroids along the chosen axis as my splitting point. This heuristic works well for most scenes because it adapts to the actual distribution of objects in the scene, is more robust than simply taking the midpoint of the bounding box, and is computationally efficient compared to more complex heuristics.</li>
            <li>Then, I partition primitives based on whether their centroids fall below or above the split point.</li>
            <li>To prevent infinite recursion, I handle the case where all primitives might end up on one side of the split by falling back to sorting primitives along the chosen axis and splitting them in the middle.</li>
            <li>Finally, I recursively build the left and right subtrees until the entire scene is organized into a hierarchical structure.</li>
        </ul>
        <p><strong>2. Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.</strong><br>
        <div class="grid-container">
            <div class="image-container">
                <h3>Wall-E</h3>
                <img src="media/2.3.png" alt="Wall-E">
            </div>
    
            <div class="image-container">
                <h3>Max Planck</h3>
                <img src="media/2.1.png" alt="Max Planck">
            </div>
    
            <div class="image-container">
                <h3>Peter</h3>
                <img src="media/2.2.png" alt="Peter">
            </div>
        </div>
        <p><strong>3. Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.</strong><br></p>
        <p> My BVH implementation dramatically improved rendering performance across all test scenes. Testing three models of increasing complexity, I found that the speedup becomes more significant as scene complexity grows. The cow model (5,856 primitives) rendered 370× faster, dropping from 25.38s to just 0.069s. The teapot (2,464 primitives) improved by 185×, from 11.25s to 0.061s. 
            Most impressive was the beast model (64,618 primitives), which saw a 5,250× speedup - from nearly 5 minutes to just 0.055s. These improvements directly correlate with the reduction in intersection tests per ray: the cow model dropped from 672 to 2.4 tests, the teapot from 307 to 2.1, and the beast from 8,575 to just 1.9. 
            Ray processing efficiency increased by over 250× in all cases, with the beast model reaching 5.65 million rays/second compared to its previous 0.0016 million. These results confirm the expected O(log n) vs. O(n) complexity improvement and show why BVH acceleration is essential for practical path tracing. Previously unrenderable scenes now process in less than a tenth of a second. </p>
    </div>
    
    <h2>Part 3: Direct Illumination</h2>
    <div class="algorithm-steps">
        <p><strong>1. Walk through both implementations of the direct lighting function.</strong><br></p>
        <p>... </p>
        <p><strong>2. Show some images rendered with both implementations of the direct lighting function.</strong><br></p>
        <div class="grid-container">
            <div class="image-container">
                <h3>Level 3</h3>
                <img src="media/1.4.png" alt="Level 3">
                <p>After three steps, we have 3 intermediate points (blue).</p>
            </div>
    
            <div class="image-container">
                <h3>Level 4</h3>
                <img src="media/1.5.png" alt="Level 4">
                <p>After four steps, we have 2 intermediate points (blue).</p>
            </div>
    
            <div class="image-container">
                <h3>Level 5</h3>
                <img src="media/1.6.png" alt="Level 5">
                <p>After five steps, we have the final point (red) that lies on the Bezier curve at parameter t.</p>
            </div>
        </div>
        <p><strong>3. Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.</strong><br></p>
        <div class="grid-container">
            <div class="image-container">
                <h3>Level 3</h3>
                <img src="media/1.4.png" alt="Level 3">
                <p>After three steps, we have 3 intermediate points (blue).</p>
            </div>
    
            <div class="image-container">
                <h3>Level 4</h3>
                <img src="media/1.5.png" alt="Level 4">
                <p>After four steps, we have 2 intermediate points (blue).</p>
            </div>
    
            <div class="image-container">
                <h3>Level 5</h3>
                <img src="media/1.6.png" alt="Level 5">
                <p>After five steps, we have the final point (red) that lies on the Bezier curve at parameter t.</p>
            </div>
        </div>
        <p><strong>4. Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.</strong><br></p>
        <p> Uniform hemisphere sampling and direct light sampling represent two fundamentally different approaches to solving the same integral in the rendering equation. Light sampling delivers superior results in almost all practical scenarios, especially for scenes with small or distant light sources. 
            With hemisphere sampling, the probability of randomly hitting a small light source is extremely low, resulting in high variance (noise) unless an impractically large number of samples are used. This is particularly evident in scenes with point lights, which hemisphere sampling essentially cannot handle. 
            Light sampling, by contrast, guarantees that every sample contributes to the final image, resulting in much lower noise for the same number of samples. The difference is most dramatic in scenes with focused lighting, where hemisphere sampling produces extremely noisy images even with high sample counts, while light sampling produces clean results with relatively few samples.
            For scenes with very large area lights or highly occluded geometries, the gap narrows somewhat, but light sampling remains the more efficient technique. The only potential advantage of hemisphere sampling is its simplicity and the fact that it can accidentally capture some indirect illumination effects (light bouncing off non-emissive surfaces), though this is better handled with explicit global illumination techniques. </p>
    </div>
        
    <h2>Part 4: Global Illumination</h2>
    <div class="algorithm-steps">
        <p><strong>1. Walk through your implementation of the indirect lighting function.</strong><br></p>
        <p>...</p>
        <p><strong>2. Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.</strong><br></p>
        <p>...</p>
        <p><strong>3. Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)</strong><br></p>
        <p>...</p>
        <p><strong>4. For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), and isAccumBounces=false. Explain in your write-up what you see for the 2nd and 3rd bounce of light, and how it contributes to the quality of the rendered image compared to rasterization. Use 1024 samples per pixel.</strong><br></p>
        <p>...</p>
        <p><strong>5. Compare rendered views of accumulated and unaccumulated bounces for CBbunny.dae with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag). Use 1024 samples per pixel.</strong><br></p>
        <p>...</p>
        <p><strong>6. Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.</strong><br></p>
        <p>...</p>
    </div>

    <h2>Part 5: Adaptive Sampling</h2>
    <div class="algorithm-steps">
        <p><strong>1. Explain adaptive sampling. Walk through your implementation of the adaptive sampling.</strong><br><p>
        <p>...</p>
        <p><strong>2. Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. 
            Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.</strong><br></p>
        <p>...</p>
    </div>
</body>
</html>
